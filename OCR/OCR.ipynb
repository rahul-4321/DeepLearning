{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2246631b-d062-4fe4-9c4d-e4bfb55b20d4",
   "metadata": {},
   "source": [
    "what is sequential model and what are its alternatives and their uses?\n",
    "ans:\n",
    "### Sequential Model\n",
    "\n",
    "The Sequential model in Keras is a linear stack of layers. It is ideal for creating simple neural networks where layers are arranged in a sequence, one after the other. Each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Suitable for simple, feed-forward neural networks.\n",
    "- Commonly used for straightforward tasks such as image classification, regression, and basic NLP tasks where the data flows in one direction (from input to output).\n",
    "\n",
    "### Alternatives to Sequential Model\n",
    "\n",
    "1. **Functional API**\n",
    "2. **Model Subclassing**\n",
    "\n",
    "#### 1. Functional API\n",
    "\n",
    "The Functional API is more flexible than the Sequential model. It allows for the creation of complex models, such as multi-input, multi-output models, shared layers, and models with non-linear topology (e.g., residual connections).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "inputs = Input(shape=(784,))\n",
    "x = Dense(32, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Useful for models with multiple inputs and outputs.\n",
    "- Enables the creation of complex architectures like Siamese networks, residual networks (ResNet), and attention mechanisms.\n",
    "- Allows the reuse of layers by sharing them across different inputs.\n",
    "\n",
    "#### 2. Model Subclassing\n",
    "\n",
    "Model subclassing provides full control over the model architecture by allowing the creation of custom models by subclassing the `tf.keras.Model` class. It is the most flexible way to create models but requires more code and a deeper understanding of the Keras framework.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = Dense(32, activation='relu')\n",
    "        self.dense2 = Dense(64, activation='relu')\n",
    "        self.dense3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "model = MyModel()\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Necessary for models with dynamic architectures that can't be defined using the Sequential model or Functional API.\n",
    "- Useful for implementing custom training loops and complex computations within the model.\n",
    "- Ideal for research and development when experimenting with novel model architectures.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Sequential Model**: Best for simple, feed-forward neural networks where layers are added sequentially.\n",
    "- **Functional API**: Offers flexibility to build complex models with multiple inputs and outputs, shared layers, and non-linear topologies.\n",
    "- **Model Subclassing**: Provides the highest level of flexibility, allowing for custom and dynamic model architectures and is ideal for advanced research and development tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a87901-b46d-44c4-adf6-bc585b1d062a",
   "metadata": {},
   "source": [
    "which is better for OCR and why not others?\n",
    "\n",
    "When considering Optical Character Recognition (OCR) tasks, the choice between Sequential models, the Functional API, and Model Subclassing in Keras depends on the complexity and requirements of the OCR system being developed. Here's an analysis of which approach is generally better suited for OCR and why:\n",
    "\n",
    "### Functional API\n",
    "\n",
    "**Why the Functional API is often the best choice for OCR:**\n",
    "\n",
    "1. **Flexibility in Architecture:**\n",
    "   - OCR systems often require complex architectures, such as Convolutional Neural Networks (CNNs) for feature extraction followed by Recurrent Neural Networks (RNNs) for sequence modeling. The Functional API allows for the design of such complex architectures easily.\n",
    "   - OCR tasks may also benefit from attention mechanisms, which can be more naturally implemented using the Functional API.\n",
    "\n",
    "2. **Multiple Inputs and Outputs:**\n",
    "   - OCR systems might need to handle multiple input sources (e.g., images and bounding box coordinates) or produce multiple outputs (e.g., character sequences and confidence scores). The Functional API supports such scenarios efficiently.\n",
    "   \n",
    "3. **Layer Sharing and Reuse:**\n",
    "   - Layers, such as feature extractors, can be shared across different parts of the network, which is straightforward with the Functional API. This can be crucial for models that process parts of an image separately but use shared weights.\n",
    "\n",
    "4. **Non-linear Topologies:**\n",
    "   - OCR models may require non-linear data flows, such as residual connections or skip connections, which are cumbersome to implement with the Sequential model but are straightforward with the Functional API.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM\n",
    "\n",
    "inputs = Input(shape=(32, 32, 1))\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = LSTM(128, return_sequences=True)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "### Why Not the Others?\n",
    "\n",
    "#### Sequential Model\n",
    "\n",
    "- **Limitations:**\n",
    "  - **Linear Topology Only:** Sequential models are limited to a linear stack of layers, which is insufficient for the complex, multi-component nature of OCR systems that often require parallel or interconnected layers.\n",
    "  - **Single Input/Output:** They are not designed to handle multiple inputs and outputs gracefully, which is often needed in OCR tasks.\n",
    "\n",
    "- **Use Case:** \n",
    "  - Can be used for very simple OCR tasks where a straightforward convolutional and dense layer stack suffices, but this is rarely the case in modern OCR applications.\n",
    "\n",
    "#### Model Subclassing\n",
    "\n",
    "- **Complexity:**\n",
    "  - **Code Overhead:** While subclassing provides maximum flexibility, it requires more boilerplate code and a deeper understanding of the framework, which can be overkill for many OCR tasks.\n",
    "  - **Custom Training Loops:** Often necessitates custom training loops and more manual management of layers and operations, which can increase development time and complexity.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Ideal for highly customized or experimental OCR models where standard layers and operations do not suffice. It is also useful when implementing novel research ideas that require dynamic changes during the forward pass.\n",
    "\n",
    "### Summary\n",
    "\n",
    "For most OCR tasks, the **Functional API** in Keras strikes the best balance between flexibility and ease of use. It allows for the construction of complex models needed for OCR, such as those incorporating CNNs, RNNs, and attention mechanisms, while maintaining a straightforward and intuitive code structure. The **Sequential Model** is too limited for the typical complexity of OCR systems, and **Model Subclassing**, while very flexible, often introduces unnecessary complexity for standard OCR tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ecd941-b719-4a72-b9db-c0fe4d76f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecd3f68-d3ce-4169-bb74-f88e95acb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data\n",
    "(x_train,y_train), (x_test, y_test)=mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749171e5-3bbe-46d7-b322-224c8b936b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train[0].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0].squeeze(),cmap='gray')\n",
    "plt.title(f'Label: {y_train[0]}')\n",
    "#plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5291b07-5e64-4a82-bb6d-5e2081ba4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the images to the range of [0,1]\n",
    "x_train=x_train.astype('float32')/255.0\n",
    "x_test=x_test.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12706075-4665-4a3c-bd81-9d18df0ce1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''since the images in the MNIST are grayscale with the shape of 28X28, we need to reshape it to include the channel \n",
    "as the deep learing model uses the images of dimension (height, width, channel)'''\n",
    "x_train=np.expand_dims(x_train,axis=-1)\n",
    "x_test=np.expand_dims(x_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521786cb-7295-440a-831e-97253e78a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-shot encode the labels\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e8aaf-07b9-471c-b3f0-b5b6d407fd90",
   "metadata": {},
   "source": [
    "step 3: Build the Model using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7062efc4-6c08-4f77-b895-f73411a88634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the inputs\n",
    "inputs=Input(shape=(28,28,1))\n",
    "\n",
    "#First Convoluitonal block\n",
    "x=Conv2D(32,kernel_size=(3,3),activation='relu')(inputs)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1ecd3-2455-4f10-8421-59b6d2584362",
   "metadata": {},
   "source": [
    "In the code snippet you provided, the operations define the input layer and the first convolutional block of a Convolutional Neural Network (CNN) using the Keras Functional API. Let’s break down each part of the code:\n",
    "\n",
    "Code Explanation\n",
    "python\n",
    "Copy code\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D\n",
    "\n",
    "# Define the inputs\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "# First Convolutional block\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "Detailed Breakdown\n",
    "1. Define the Inputs\n",
    "python\n",
    "Copy code\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "Input(shape=(28, 28, 1)): This defines the input layer of the model.\n",
    "shape=(28, 28, 1): The shape parameter specifies the dimensions of the input data.\n",
    "28, 28: These dimensions correspond to the height and width of the MNIST images.\n",
    "1: This dimension corresponds to the single color channel, as MNIST images are grayscale.\n",
    "2. First Convolutional Block\n",
    "python\n",
    "Copy code\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "Conv2D(32, kernel_size=(3, 3), activation='relu'): This creates a convolutional layer.\n",
    "32: The number of filters (also known as kernels) to be used in this layer. Each filter will produce a separate output feature map.\n",
    "kernel_size=(3, 3): The dimensions of the filters (3x3 pixels). This means that each filter will look at a 3x3 patch of the input image.\n",
    "activation='relu': The ReLU (Rectified Linear Unit) activation function is applied to introduce non-linearity into the model.\n",
    "(inputs): This layer takes the inputs tensor as its input.\n",
    "python\n",
    "Copy code\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "MaxPooling2D(pool_size=(2, 2)): This creates a max-pooling layer.\n",
    "pool_size=(2, 2): The dimensions of the pooling window (2x2 pixels). This means that each 2x2 region in the feature map will be downsampled to a single value (the maximum value in that region).\n",
    "(x): This layer takes the output tensor x from the previous convolutional layer as its input.\n",
    "What These Layers Do\n",
    "Input Layer: Defines the shape and structure of the input data.\n",
    "Conv2D Layer:\n",
    "Applies 32 filters to the input image, where each filter convolves over the image to produce a set of feature maps.\n",
    "The 3x3 filters detect various features in the input image such as edges, textures, etc.\n",
    "The ReLU activation function introduces non-linearity, which allows the network to learn more complex patterns.\n",
    "MaxPooling2D Layer:\n",
    "Reduces the spatial dimensions (height and width) of the feature maps by taking the maximum value in each 2x2 window.\n",
    "This downsampling helps reduce the computational complexity and also makes the feature detection more robust to small translations in the input.\n",
    "Summary\n",
    "The provided code defines the input layer and the first convolutional block of a CNN. The input layer specifies that the input images are 28x28 pixels with a single channel. The convolutional layer applies 32 filters of size 3x3 to the input image and then applies the ReLU activation function. The max-pooling layer then reduces the spatial dimensions of the feature maps by taking the maximum value in each 2x2 window, which helps in reducing the complexity and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26adef96-cf96-467f-9483-e03d8586cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second convolutional block\n",
    "x=Conv2D(64,kernel_size=(3,3),activation='relu')(x)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7543288a-33b5-4ea5-bed5-458479fcbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the output and add the fully connected layers\n",
    "x=Flatten()(x)\n",
    "x=Dense(128,activation='relu')(x)\n",
    "x=Dropout(0.5)(x) #Droupout for regulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022eb239-d4b4-4d26-95b3-f76c39425da8",
   "metadata": {},
   "source": [
    "In the given code, the operations flatten the output from the convolutional layers and then add fully connected (dense) layers to the model. Let's break down each step:\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Flatten the output\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a fully connected layer\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Add dropout for regularization\n",
    "x = Dropout(0.5)(x)\n",
    "```\n",
    "\n",
    "### Detailed Breakdown\n",
    "\n",
    "#### 1. Flatten the Output\n",
    "\n",
    "```python\n",
    "x = Flatten()(x)\n",
    "```\n",
    "\n",
    "- `Flatten()`: This layer converts the multi-dimensional output from the previous layer (usually the output from convolutional or pooling layers) into a 1D vector.\n",
    "- `(x)`: This takes the output tensor `x` from the previous max-pooling layer as its input.\n",
    "  \n",
    "For example, if the output shape from the previous layer is `(batch_size, 13, 13, 32)`, the `Flatten` layer will convert it to `(batch_size, 13 * 13 * 32)`, resulting in a 1D vector for each sample in the batch.\n",
    "\n",
    "#### 2. Add a Fully Connected Layer\n",
    "\n",
    "```python\n",
    "x = Dense(128, activation='relu')(x)\n",
    "```\n",
    "\n",
    "- `Dense(128, activation='relu')`: This creates a fully connected (dense) layer.\n",
    "  - `128`: The number of neurons (units) in this layer. This means that the output vector will have 128 dimensions.\n",
    "  - `activation='relu'`: The ReLU (Rectified Linear Unit) activation function is applied to each neuron in this layer, introducing non-linearity to the model.\n",
    "  - `(x)`: This layer takes the 1D vector `x` from the `Flatten` layer as its input.\n",
    "\n",
    "Fully connected layers are used to combine all the features learned by the convolutional layers and apply the final classification logic. Each neuron in this layer is connected to every neuron in the previous layer, hence the name \"fully connected.\"\n",
    "\n",
    "#### 3. Add Dropout for Regularization\n",
    "\n",
    "```python\n",
    "x = Dropout(0.5)(x)\n",
    "```\n",
    "\n",
    "- `Dropout(0.5)`: This creates a dropout layer.\n",
    "  - `0.5`: The dropout rate, meaning that 50% of the neurons will be randomly set to 0 during each training step. This is done to prevent overfitting.\n",
    "  - `(x)`: This layer takes the output tensor `x` from the previous dense layer as its input.\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting by randomly \"dropping out\" (setting to zero) a fraction of neurons during training. This helps the model to generalize better by ensuring that the network does not become overly reliant on any particular neurons.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The provided code performs the following steps:\n",
    "\n",
    "1. **Flatten Layer**: Converts the 3D tensor output from the convolutional or pooling layers into a 1D vector. This prepares the data for the fully connected layers.\n",
    "2. **Dense Layer**: Adds a fully connected layer with 128 neurons and ReLU activation. This layer processes the flattened vector and helps in combining the features learned by the convolutional layers.\n",
    "3. **Dropout Layer**: Adds a dropout layer with a dropout rate of 50%. This randomly sets half of the neurons to zero during training to prevent overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "This structure is typically used in CNNs to combine feature extraction (via convolutional layers) with classification (via fully connected layers) while incorp\n",
    "\n",
    "\n",
    "why is dropout only 0.5?\n",
    "how to choose the value?\n",
    "\n",
    "The dropout rate of 0.5 is commonly used as a default value in many deep learning applications because it has been found to be effective in preventing overfitting while allowing the model to learn. However, the optimal dropout rate can vary depending on the specific problem, model architecture, and dataset.\n",
    "\n",
    "### Why 0.5 for Dropout?\n",
    "- **Empirical Success**: Research and practical experience have shown that a dropout rate of 0.5 often works well in a variety of scenarios. This means that 50% of the neurons are randomly set to zero during each training step, which helps to prevent the network from becoming too reliant on any particular neurons.\n",
    "- **Balance**: A dropout rate of 0.5 provides a good balance between retaining enough neurons to learn useful features and dropping enough neurons to prevent overfitting.\n",
    "\n",
    "### How to Choose the Dropout Rate?\n",
    "The choice of the dropout rate should be determined through experimentation and cross-validation. Here are some guidelines:\n",
    "\n",
    "1. **Start with Common Values**: \n",
    "   - For fully connected (dense) layers, starting with 0.5 is a good practice.\n",
    "   - For convolutional layers, smaller dropout rates such as 0.2 or 0.3 are often used because convolutional layers typically have fewer parameters compared to fully connected layers.\n",
    "\n",
    "2. **Experiment and Tune**:\n",
    "   - Use cross-validation or a validation set to test different dropout rates and evaluate their impact on the model’s performance.\n",
    "   - Typical values to try include 0.2, 0.3, 0.4, 0.5, and 0.6. Higher values might also be tested but often lead to underfitting.\n",
    "\n",
    "3. **Consider the Complexity of the Model and Dataset**:\n",
    "   - If the model is very large or the dataset is relatively small, a higher dropout rate might be necessary to prevent overfitting.\n",
    "   - Conversely, for smaller models or larger datasets, a lower dropout rate might be sufficient.\n",
    "\n",
    "### Example: Adjusting Dropout Rate\n",
    "\n",
    "Here's how you can adjust the dropout rate in the provided code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Flatten the output\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a fully connected layer\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rate = 0.5  # Start with 0.5, but you can try other values such as 0.3 or 0.6\n",
    "x = Dropout(dropout_rate)(x)\n",
    "```\n",
    "\n",
    "### Experimentation Example\n",
    "\n",
    "You might create a loop or grid search to find the optimal dropout rate:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Split into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create the model with different dropout rates\n",
    "def create_model(dropout_rate):\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "results = {}\n",
    "for rate in dropout_rates:\n",
    "    model = create_model(rate)\n",
    "    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), batch_size=128)\n",
    "    results[rate] = history.history\n",
    "\n",
    "# Display results\n",
    "for rate, history in results.items():\n",
    "    print(f\"Dropout rate: {rate}, Validation accuracy: {history['val_accuracy'][-1]}\")\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Starting Point**: Use 0.5 for dense layers and slightly lower values for convolutional layers.\n",
    "- **Experimentation**: Test different rates using cross-validation to find the optimal value for your specific model and dataset.\n",
    "- **Balance**: Ensure you balance between preventing overfitting and maintaining enough capacity to learn from the data.orating regularization techniques to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97480c3-0843-46c4-98dd-93850f16d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "outputs=Dense(10,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1b77af-8eb6-404a-b5c2-ac43310e70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model=Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa9923-b7eb-4c6f-a243-5d6658b583d3",
   "metadata": {},
   "source": [
    "Step-4: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee7fee0-b253-4e20-96ea-ef54ed3961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d17d77-e5e5-42d6-9e9a-01871a3f1519",
   "metadata": {},
   "source": [
    "step-5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf1bdd12-f92a-4974-adeb-e3ceeb79e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 16s 42ms/step - loss: 0.3606 - accuracy: 0.8905 - val_loss: 0.0847 - val_accuracy: 0.9762\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 15s 41ms/step - loss: 0.1118 - accuracy: 0.9669 - val_loss: 0.0607 - val_accuracy: 0.9830\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 15s 41ms/step - loss: 0.0847 - accuracy: 0.9743 - val_loss: 0.0474 - val_accuracy: 0.9858\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 16s 42ms/step - loss: 0.0656 - accuracy: 0.9801 - val_loss: 0.0442 - val_accuracy: 0.9877\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 16s 42ms/step - loss: 0.0553 - accuracy: 0.9829 - val_loss: 0.0416 - val_accuracy: 0.9873\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 16s 42ms/step - loss: 0.0494 - accuracy: 0.9851 - val_loss: 0.0387 - val_accuracy: 0.9888\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 16s 43ms/step - loss: 0.0427 - accuracy: 0.9873 - val_loss: 0.0349 - val_accuracy: 0.9901\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 16s 43ms/step - loss: 0.0373 - accuracy: 0.9888 - val_loss: 0.0361 - val_accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 16s 44ms/step - loss: 0.0364 - accuracy: 0.9891 - val_loss: 0.0365 - val_accuracy: 0.9897\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 17s 44ms/step - loss: 0.0304 - accuracy: 0.9903 - val_loss: 0.0343 - val_accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6ab87-041d-4bbf-a686-ba272ca91001",
   "metadata": {},
   "source": [
    "step-6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3d9069-8d65-4752-8da1-328801bd62b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0251 - accuracy: 0.9922\n",
      "Test accuracy:0.9922000169754028\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(x_test,y_test)\n",
    "print(f'Test accuracy:{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa684462-bb00-4147-ad8d-d72605907b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, you can save the model for future use\n",
    "#model.save('mnist_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a5b30-7e64-4fab-9895-94da9f803f85",
   "metadata": {},
   "source": [
    "step-7: Testing the model with custom images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a05dd95a-724e-4ab9-b2b2-76e0bdb9e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img=Image.open(image_path).convert('L')  #convert image to grayScale\n",
    "    #resize the image to 28 X 28\n",
    "    img=img.resize((28,28))\n",
    "    #convert the image to numpy array\n",
    "    img_array=np.array(img)\n",
    "    #normalize the image array\n",
    "    img_array=img_array.astype('float32')/255.0\n",
    "    #reshape the image to match the input shape (28,28,1)\n",
    "    img_array=np.expand_dims(img_array,axis=-1)\n",
    "    img_array=np.expand_dims(img_array,axis=0)#add batch dimension\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e916ef1-5276-4e29-9cb3-3b67d6bc8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    #preprocess the image\n",
    "    img_array=preprocess_image(image_path)\n",
    "    #make prediction\n",
    "    prediction=model.predict(img_array)\n",
    "    predict_digit=np.argmax(prediction)\n",
    "    #plot the image and the prediction image\n",
    "    plt.imshow(img_array.squeeze(),cmap='gray')\n",
    "    plt.title(f'predicted Digit: {predict_digit}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return predict_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6fd7f43-86c1-47f1-a0d4-2d06eaa8664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from PIL import Image\n",
    "image_path = 'G:\\\\Deep Learning\\\\OCR\\\\c2_0.jpg'\n",
    "predicted_digit = predict_image(image_path)\n",
    "print(f'Predicted digit: {predicted_digit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43509e16-f875-448f-90a1-8c7c338aaab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 1\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image_path = 'G:\\\\Deep Learning\\\\OCR\\\\c1_0.jpg'\n",
    "predicted_digit = predict_image(image_path)\n",
    "print(f'Predicted digit: {predicted_digit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad19d5-cad3-4c7f-ad2a-69e356c3acfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
